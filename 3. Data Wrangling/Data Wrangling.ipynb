{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alfendio Alif Faudisyah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicoding Indonesia - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengenalan Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merupakan sebuah proses atau kumpulan kegiatan yang meliputi pengumpulan data (Gathering data), penilaian data (Assessing data), serta pembersihan data (Cleaning data) sebelum data digunakan dalam proses analisis data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sama halnya dengan tahapan dalam analisis data, tahapan dalam data wrangling juga bersifat iterative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap data wrangling, dimulai dengan proses pengumpulan data. Pada proses ini kita akan mengumpulkan semua data yang dibutuhkan untuk menjawab semua pertanyaan atau masalah bisnis yang ingin kita hadapi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berbagai Sumber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle\n",
    "- UCI Machine Learning Repository\n",
    "- Google Dataset Search\n",
    "- Satu Data Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teknik pengumpulan data lain yang dapat dilakukan yaitu web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membaca Berbagai Tipe Data Menggunakan Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang telah kita bahas sebelumnya, pandas merupakan sebuah library Python yang spesifik digunakan untuk memanipulasi dan menganalisis data. Dalam mendukung hal tersebut, pandas menyediakan beberapa function yang dapat digunakan untuk membaca atau mengakses data menjadi sebuah DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format berkas CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berkas CSV (Comma Separated Values) merupakan format berkas data tabel yang paling sering digunakan dan telah menjadi standar dalam industri. Ia menggunakan koma (,) sebagai pemisah (sering disebut sebagai delimiter) antar nilai dalam satu baris.\n",
    "\n",
    "Pandas menyediakan sebuah function read_csv()untuk membaca berkas CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_csv(\"data.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format berkas XLSX atau XLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berkas XLSX atau XLS merupakan format berkas spreadsheet yang dibuat menggunakan aplikasi Microsoft Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_excel(\"data.xlsx\", sheet_name=\"Sheet1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format berkas JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berkas JSON (JavaScript Object Notation) merupakan format berkas data lain yang paling sering digunakan di industri. Ia sering digunakan karena berukuran kecil, mudah dibaca dan ditulis oleh manusia, serta mudah diproses oleh mesin. JSON memiliki struktur data yang mirip seperti data structure dictionary dalam Python yang terdiri dari pasangan keys dan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_json(\"data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format berkas HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML atau dikenal juga sebagai HyperText Markup Language merupakan sebuah markup language standar yang digunakan untuk merancang tampilan sebuah dokumen/halaman di web browser. Untuk membaca berkas ini, pandas menyediakan function read_html(). Ia akan menerima inputan berupa HTML string, HTML file, atau URL dan akan mengurai tabel HTML ke dalam bentuk list. List ini berisi kumpulan DataFrame yang diurai dari tabel HTML tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\n",
    "df = pd.read_html(url)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format berkas XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format data selanjutnya yang akan kita bahas ialah XML. Ia merupakan singkatan dari Extensible Markup Language yang sering digunakan untuk merepresentasikan berbagai struktur informasi, seperti dokumen, data, konfigurasi, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Akses data dari SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain membaca data dari berbagai format, library pandas juga memungkinkan kita untuk mengakses data langsung dari sebuah database, seperti PostgreSQL, MySQL, dll. Tentunya untuk mengakses database tersebut kita membutuhkan library pendukung yaitu SQLAlchemy.\n",
    "\n",
    "Untuk berinteraksi dengan database, pandas menyediakan tiga function seperti berikut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read_sql_table()untuk membaca SQL database table dan mempresentasikannya ke dalam bentuk pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    " \n",
    "db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\n",
    " \n",
    "pd.read_sql_table(\"table_name\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read_sql_query()untuk membaca SQL query dan mempresentasikannya ke dalam bentuk pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    " \n",
    "db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\n",
    " \n",
    "pd.read_sql_query(\"SELECT * FROM table_name\", db)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read_sql()untuk membaca SQL query atau table dan mempresentasikannya ke dalam bentuk pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    " \n",
    "db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\n",
    " \n",
    "pd.read_sql(\"SELECT * FROM table_name\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lengkapnya lihat di https://pandas.pydata.org/docs/user_guide/io.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggabungkan Beberapa Data Menjadi Satu DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu teknik penggabungan data yang paling sering digunakan ialah merge atau join. Ia merupakan teknik untuk menggabungkan dua tabel data menggunakan primary key (PK) dan foreign key (FK).  \n",
    "\n",
    "Primary key merupakan sebuah kolom dengan nilai unik yang merepresentasikan suatu data dalam sebuah tabel. Di lain sisi, foreign key merupakan kolom yang berisi primary key dari tabel lain. Ia digunakan untuk mereferensikan data dari tabel lain hingga terbentuk sebuah relationship antar tabel. Inilah yang menjadi kunci dalam relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan cara penggabungannya, proses merge atau join dapat dibagi menjadi empat jenis yaitu seperti berikut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inner\n",
    "\n",
    "Inner join merupakan proses join yang hanya mengambil nilai yang bersesuaian di kedua tabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Left\n",
    "\n",
    "Left join merupakan proses join yang akan mengambil semua nilai dari tabel kiri beserta nilai yang bersesuaian dari tabel kanan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Right\n",
    "\n",
    "Right join merupakan proses join yang akan mengambil semua nilai dari tabel kanan beserta nilai yang bersesuaian dari tabel kiri. Ia merupakan kebalikan dari left join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outer\n",
    "\n",
    "Outer join atau sering juga disebut full outer join merupakan proses join yang akan mengambil semua nilai dari kedua tabel. Ia merupakan gabungan dari left dan right join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebagai tool andalan dalam pengolahan dan analisis data, pandas menyediakan sebuah function bernama merge(). Ia dapat digunakan untuk menggabungkan dua buah DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "product_df = pd.read_csv(\"product.csv\")\n",
    "orders_df = pd.read_csv(\"orders.csv\")\n",
    " \n",
    "new_order_df = pd.merge(\n",
    "    left=product_df,\n",
    "    right=orders_df,\n",
    "    how=\"inner\",\n",
    "    left_on=\"product_id\",\n",
    "    right_on=\"product_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas akan menghasilkan sebuah DataFrame baru yang hanya mengambil nilai yang bersesuaian dari kedua DataFrame (product_df dan orders_df) tersebut. Proses ini dilakukan dengan menyesuaikan nilai pada kolom product_id yang berperan sebagai primary key dari product_df dan foreign key dari orders_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah semua data yang dibutuhkan terkumpul, proses selanjutnya ialah penilaian terhadap data tersebut. Proses ini dilakukan untuk menilai kualitas dan struktur dari sebuah data. Selain itu, proses ini juga bertujuan untuk mengidentifikasi berbagai masalah yang terdapat dalam data, seperti missing value, unstandard value, dll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data yang telah dikumpulkan harus diperiksa terlebih dahulu sebelum masuk ke tahap analisis. Pemeriksaan data ini dilakukan dengan menjalankan proses assessing data. Ia merupakan proses yang bertujuan untuk mengidentifikasi masalah yang terdapat dalam data dan memastikan data tersebut berkualitas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masalah Umum Dijumpai dalam Sebuah Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merupakan salah satu masalah yang paling sering dijumpai dalam proyek analisis data di industri. Masalah ini muncul karena adanya nilai yang hilang dari sebuah data dan biasanya direpresentasikan sebagai nilai NaN dalam library pandas. Hal ini biasanya terjadi karena adanya human error, masalah privasi, proses merging/join, dll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library pandas menyediakan sebuah method bernama isnull() atau isna() untuk mengidentifikasi missing value dalam sebuah DataFrame. Keduanya sering dipadukan dengan method sum()untuk menghitung total missing value pada setiap kolom dalam sebuah DataFrame. Berikut merupakan contoh penerapan kodenya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "product_df = pd.read_csv(\"product.csv\")\n",
    " \n",
    "products_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gambar di bawah ini merupakan contoh keluaran dari hasil pengecekan missing value. Pada contoh ini, kita menemukan cukup banyak missing value pada kolom product_category_name, product_name_lenght, dan product_description_lenght."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invalid value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masalah ini muncul ketika terdapat beberapa nilai yang tidak masuk akal, tidak sesuai dengan ketentuan, dan background knowledge dari data tersebut. Sebagai contoh, data customer id haruslah bersifat unik dan memenuhi ketentuan tertentu seperti jumlah karakter serta komposisinya. Jika terdapat customer id yang tidak memenuhi ketentuan tersebut, akan dianggap sebagai invalid value.\n",
    "\n",
    "Untuk mendeteksi masalah seperti ini, kita membutuhkan teknik sedikit advance. Salah satu teknik yang paling sering digunakan ialah teknik filtering data menggunakan regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merupakan masalah lain yang umum dijumpai di industri. Ia terjadi ketika terdapat sebuah observasi (semua nilai dalam satu unit baris) yang memiliki nilai yang sama persis pada setiap kolomnya.\n",
    "\n",
    "Pandas menyediakan sebuah method duplicated() untuk mengidentifikasi apakah terdapat duplikasi pada sebuah DataFrame. Berikut merupakan contoh penerapannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\n",
    "df = pd.read_html(url)[0]\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inaccurate value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merupakan masalah yang muncul ketika nilai dalam sebuah data tidak sesuai dengan hasil observasi. Masalah ini umumnya muncul karena adanya human error atau sistem error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adalah masalah yang muncul ketika sebuah data memiliki nilai yang tidak konsisten baik dari segi satuan maupun ketentuan penilaian. Inkonsistensi ini umumnya muncul karena adanya perbedaan standar dalam proses pengumpulan nilai.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier atau dalam bahasa indonesia disebut pencilan merupakan titik data yang berada sangat jauh dari titik data yang lain dalam sebuah dataset. Nilai yang sangat jauh ini tentunya akan berdampak terhadap beberapa parameter statistik yang digunakan untuk menganalisis data, seperti nilai mean dan standard deviation.\n",
    "\n",
    "Terdapat beberapa metode yang dapat digunakan untuk mengidentifikasi outlier dalam sebuah dataset. Metode yang paling sering digunakan ialah IQR method.\n",
    "\n",
    "IQR method merupakan metode penentuan outlier berdasarkan nilai interquartile range (IQR). Ia mengidentifikasi outlier dengan cara membuat nilai cut-off sebagai faktor k (Umumnya kita menggunakan nilai 1.5 s/d 3) dari nilai IQR. Cut-off tersebut selanjutnya akan digunakan untuk menghitung nilai ambang batas (boundary values). Nilai tersebut dibagi menjadi dua yaitu ambang batas minimum dan maksimum. Semua titik data lebih kecil dari ambak batas minimum atau lebih besar dari ambang batas maksimum akan dianggap sebagai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "q25, q75 = np.percentile(data, 75), np.percentile(data, 25)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "minimum, maximum = q25 - cut_off, q75 + cut_off\n",
    " \n",
    "outliers = [x for x in data if x < minimum or x > maximum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metode lain yang bisa digunakan ialah box plot. Ia merupakan bentuk visual untuk merepresentasikan nilai IQR beserta ambang batas bawah dan atas dari sebuah data. Hal ini tentunya akan membantu kita mengidentifikasi outlier secara lebih mudah yaitu melalui bentuk visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila pada proses sebelumnya kita menemukan masalah (missing value, outlier, dll.) yang terdapat di dalam sebuah data, masalah tersebut harus dibersihkan sebelum masuk tahap analisis data. Terdapat beberapa teknik yang dapat kita gunakan untuk membersihkan data. Seluruh teknik tersebut akan kita pelajari pada beberapa materi ke depan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secara umum, proses pembersihan data dapat dibagi ke dalam tiga tahapan, yaitu:\n",
    "\n",
    "- Define: pada tahap ini, kita akan membuat rancangan tahapan serta metode pembersihan data berdasarkan masalah yang ditemukan dalam proses assessing data. Hal ini dapat dijadikan sebagai dokumentasi untuk memastikan orang lain memahami setiap tahapan dalam pembersihan data yang akan kita lakukan.\n",
    "\n",
    "- Code: setelah membuat rancangan pembersihan data, tahap selanjutnya ialah mengonversi hal tersebut menjadi sebuah kode program yang dapat dijalankan.\n",
    "\n",
    "- Test: setelah menjalankan kode program untuk membersihkan data, kita perlu memeriksa kembali data yang telah dibersihkan tersebut. Hal ini untuk memastikan proses pembersihan data dilakukan sesuai ekspektasi kita. \n",
    "\n",
    "\n",
    "Ketiga tahapan tersebut telah menjadi standar best practice dalam proses pembersihan data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teknik untuk Mengatasi Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping\n",
    "\n",
    "Metode pertama dan yang paling mudah dalam mengatasi missing value adalah dropping. Pada metode ini, kita akan menghapus seluruh baris atau kolom yang memiliki missing value. Untuk melakukannya, kita bisa menggunakan salah satu method dropna()yang disediakan oleh library pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "products_df = pd.read_csv(\"product.csv\")\n",
    " \n",
    "products_df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada contoh kode di atas, parameter axis=0 (menerima nilai 0 atau 1) menandakan kita ingin men-drop seluruh baris yang mengandung missing value. inplace=True menandakan kita ingin langsung menerapkan operasi tersebut ke dalam DataFrame products_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baca dokumentasi https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metode dropping ini memang terkesan mudah, tetapi perlu Anda ketahui bahwa ada banyak sekali pertimbangan yang perlu diperhatikan sebelum menggunakan metode ini. Salah satu pertimbangan penting ialah penerapan metode ini dapat menyebabkan kita kehilangan banyak informasi. Bergantung pada kasus dan jumlah data yang sedang ditangani, terkadang kita bisa mengabaikan hilangnya informasi ini. Sebagai contoh, jika bekerja dengan dataset yang sangat besar (puluhan ribu), kita bisa mengabaikan dampak tersebut. Sebaliknya, ketika bekerja menangani data berbentuk time series (data yang disusun berdasarkan urutan waktu) atau ukuran data yang kecil, kita perlu mempertimbangkan lagi penggunaan metode ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imputation\n",
    "\n",
    "Metode lain yang umum digunakan untuk mengatasi missing value ialah metode imputation. Metode ini bekerja dengan cara mengisi (fill) missing value dengan nilai tertentu. Hal ini tentunya akan mencegah hilangnya informasi akibat missing value.\n",
    "\n",
    "Pada data kontinu, kita bisa menggunakan nilai mean, median, atau mode sebagai pengganti missing value. Jika bekerja menggunakan data kategoris, kita dapat mengisi missing value dengan kategori yang paling sering muncul. Namun, perlu diingat bahwa pemilihan nilai pengganti ini harus didukung oleh background knowledge dari data tersebut. Pada beberapa kasus ada suatu nilai tertentu yang digunakan untuk mengganti missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebagai library andalan kita, pandas telah menyediakan sebuah method bernama fillna() untuk mengganti missing value dengan nilai tertentu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baca dokumentasi https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut merupakan contoh kode untuk mengganti missing value pada kolom agedengan nilai mean dari kolom tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "data=pd.read_csv('employee_data.csv')\n",
    " \n",
    "data.age.fillna(value=data.age.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metode ini masih memiliki banyak kekurangan salah satunya ialah dapat mempengaruhi variance atau sebaran dari sebuah data. Selain itu, metode ini juga masih belum cukup baik untuk diterapkan pada data time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpolation\n",
    "\n",
    "Metode penanganan missing value terakhir yang akan kita bahas ialah interpolation (interpolasi). Sederhananya, interpolasi merupakan salah satu pendekatan numerik yang digunakan untuk menghitung titik data baru berdasarkan range data yang sudah ada. Perhitungan ini menggunakan sebuah persamaan garis linear ataupun polynomial. Perhitungan tersebut membuat metode ini sangat cocok digunakan untuk menangani missing value pada data time series.\n",
    "\n",
    "Library pandas juga menyediakan method interpolate() yang bisa kita gunakan untuk menerapkan metode interpolasi dalam mengatasi missing value. Ketika menggunakan method ini, kita perlu mendefinisikan metode interpolasi yang ingin digunakan, seperti linear, polynomial, dll. Selain itu, kita juga perlu mendefinisikan parameter limit_direction (forward, backward, dan both) untuk menspesifikkan arah konstruktif dari proses interpolasi. Berikut merupakan contoh kode untuk menggunakan method interpolate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "data=pd.read_csv('bbca_index.csv')\n",
    " \n",
    "data.close_price.interpolate(method='linear', limit_direction='forward', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baca dokumentasi https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teknik untuk Mengatasi Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada dasarnya, terdapat dua metode yang umum digunakan untuk mengatasi outlier yaitu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop\n",
    "\n",
    "Metode pertama yang paling mudah ialah men-drop atau menghapus seluruh baris yang mengandung outlier. Metode ini mampu mencegah outlier mempengaruhi hasil analisis yang kita buat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_csv(\"data.csv\")\n",
    " \n",
    "Q1 = (df['TotalCharges']).quantile(0.25)\n",
    "Q3 = (df['TotalCharges']).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    " \n",
    "maximum = Q3 + (1.5*IQR)\n",
    "minimum = Q1 - (1.5*IQR)\n",
    " \n",
    "kondisi_lower_than = df['TotalCharges'] < maximum\n",
    "kondisi_more_than = df['TotalCharges'] > minimum\n",
    " \n",
    "df.drop(df[kondisi_lower_than].index, inplace=True)\n",
    "df.drop(df[kondisi_more_than].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imputation\n",
    "\n",
    "Konsepnya mirip seperti sebelumnya yaitu mengganti outlier dengan nilai tertentu. Nilai yang bisa kita gunakan ialah mean, media, dan mode. Selain itu, tidak jarang juga kita mengganti outlier dengan boundary value.\n",
    "\n",
    "Untuk menerapkan metode ini, kita bisa menggunakan method mask() yang disediakan oleh library pandas. Method tersebut menerima parameter cond sebagai kondisi untuk memfilter nilai outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    " \n",
    "Q1 = (df['TotalCharges']).quantile(0.25)\n",
    "Q3 = (df['TotalCharges']).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    " \n",
    "maximum = Q3 + (1.5*IQR)\n",
    "minimum = Q1 - (1.5*IQR)\n",
    " \n",
    "kondisi_lower_than = df['TotalCharges'] < minimum\n",
    "kondisi_more_than = df['TotalCharges'] > maximum\n",
    " \n",
    "df.mask(cond=kondisi_more_than, maximum, axis=1, inplace=True)\n",
    "df.mask(cond=kondisi_lower_than, minimum, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teknik untuk Mengatasi Duplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika jumlah duplikasi dan ukuran data masih sedikit, mungkin kita bisa menghapusnya secara manual. Namun, solusi ini akan sangat merepotkan ketika jumlah duplikasi dan ukuran data membesar. Oleh karena itu, solusi seperti ini tidaklah scalable.\n",
    "\n",
    "Library pandas telah menyediakan sebuah method drop_duplicates() untuk menghilangkan duplikasi dalam sebuah DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.drop_duplicates(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
